{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (4600,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets import load_spam\n",
    "\n",
    "X, y = load_spam()\n",
    "\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9096d",
   "metadata": {},
   "source": [
    "# Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X, y = load_spam()\n",
    "\n",
    "missing_values = np.isnan(X).sum()\n",
    "print(missing_values)\n",
    "\n",
    "\n",
    "if missing_values.any() > 0:\n",
    "    X = np.nan_to_num(X, nan=np.nanmean(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_small - Shape: (230, 57)\n",
      "y_small - Shape: (230,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_small, y_train, y_small = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "print(\"X_small - Shape:\", X_small.shape)\n",
    "print(\"y_small - Shape:\", y_small.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3732f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7f2a1",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Data size  Training accuracy  Validation accuracy\n",
      "0     Full Dataset           0.927174             0.938043\n",
      "1  Partial Dataset           0.614946             0.593478\n",
      "2    Small Dataset           0.940217             0.847826\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yellowbrick.datasets import load_spam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "results = []\n",
    "\n",
    "X, y = load_spam()\n",
    "X = np.array(X)  \n",
    "\n",
    "datasets = [\n",
    "    (\"Full Dataset\", X, y),\n",
    "    (\"Partial Dataset\", X[:, :2], y),\n",
    "    (\"Small Dataset\", X_small, y_small)\n",
    "]\n",
    "\n",
    "for dataset_name, X_data, y_data in datasets:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    results.append([dataset_name, accuracy_train, accuracy_val])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Data size\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "\n",
    "Training Accuracy - As you reduce the amount of data used, training accuracy tends to increase because with less data, the model may overfit, fitting noise instead of true patterns in the data.\n",
    "\n",
    "Validation Accuracy - As you reduce the amount of data used, validation accuracy may decrease because the model may struggle to generalize new data due to less validation data. In other words, the smaller the validation dataset, the less confident you can be in the model's ability to generalize.\n",
    "\n",
    "For example:\n",
    "\n",
    "Original Dataset:\n",
    "Training Accuracy: 0.927174\n",
    "Validation Accuracy: 0.938043\n",
    "\n",
    "First Two Columns of the Dataset:\n",
    "Training Accuracy: 0.614946\n",
    "Validation Accuracy: 0.593478\n",
    "\n",
    "Smaller Dataset:\n",
    "Training Accuracy: 0.940217\n",
    "Validation Accuracy: 0.847826\n",
    "\n",
    "As you can see, the validation accuracy decreases whereas the training accuracy increases as less data is being used.\n",
    "\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "False Positive - In this case, it means when a legitimate email is classified as spam and may end up in the spam folder.\n",
    "\n",
    "False Negative - In this case, it means a spam email is classified as legitimate and may end up in the inbox.\n",
    "\n",
    "In this case, false positives are worse because an important email may be disregarded due to being classified as spam. This is more severe than the alternative which would be a spam email being flagged as legitimate. Although the latter could be annoying, it is likely to have less severe consequences.\n",
    "\n",
    "\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "From the examples, lectures provided in class, as well as online tools and libraries.\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "In the order that they were listed.\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "\n",
    "I used generative AI to better understand the questions and expectations for the solutions. The prompts were more or less the questions. I also used generative AI when I was experiencing errors, especially for steps 3-5. It helped me figure out what was going wrong.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "I did have challenges remembering how to perform certain functions. I had to dig through some of the old examples or search online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size and type of X:\n",
      "(1030, 8) <class 'pandas.core.frame.DataFrame'>\n",
      "Size and type of y:\n",
      "(1030,) <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()\n",
    "\n",
    "print(\"Size and type of X:\")\n",
    "print(X.shape, type(X))\n",
    "\n",
    "print(\"Size and type of y:\")\n",
    "print(y.shape, type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values in the dataset.\n",
      "Missing values have been handled.\n"
     ]
    }
   ],
   "source": [
    "missing_values_X = X.isnull().sum().sum()\n",
    "missing_values_y = y.isnull().sum()\n",
    "\n",
    "if missing_values_X == 0 and missing_values_y == 0:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(f\"Missing values in X: {missing_values_X}\")\n",
    "    print(f\"Missing values in y: {missing_values_y}\")\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "if X.isnull().sum().sum() == 0 and y.isnull().sum() == 0:\n",
    "    print(\"Missing values have been handled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5041945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model with the data\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error: 110.66177124473455\n",
      "Validation Mean Squared Error: 95.97548435336684\n",
      "Training R2 Score: 0.6104593527939581\n",
      "Validation R2 Score: 0.6275416055429188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and validation data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Calculate mean squared error for training and validation\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate R2 score for training and validation\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "print(f\"Training Mean Squared Error: {mse_train}\")\n",
    "print(f\"Validation Mean Squared Error: {mse_valid}\")\n",
    "print(f\"Training R2 Score: {r2_train}\")\n",
    "print(f\"Validation R2 Score: {r2_valid}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Training accuracy Validation accuracy\n",
      "MSE             110.661771           95.975484\n",
      "R2 score          0.610459            0.627542\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate mean squared error for training and validation\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_valid = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "# Calculate R2 score for training and validation\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_valid = r2_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "results = pd.DataFrame(columns=['Training accuracy', 'Validation accuracy'], index=['MSE', 'R2 score'])\n",
    "results.at['MSE', 'Training accuracy'] = mse_train\n",
    "results.at['MSE', 'Validation accuracy'] = mse_valid\n",
    "results.at['R2 score', 'Training accuracy'] = r2_train\n",
    "results.at['R2 score', 'Validation accuracy'] = r2_valid\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "I believe using a linear model produced decent results. Although the MSE values are considerably high (110.661771 & 95.975484) which indicate a significant amount of variance that the linear model is not capturing well, the validation MSE is slightly lower than the training MSE which may suggest that the model generalizes reasonably well to unseen data.\n",
    "\n",
    "The R2 scores of 0.610459 & 0.627542 are not extremely close to 1 but they indicate that the linear model explains a reasonable portion of the variance in the data.\n",
    "\n",
    "Therefore, the results indicate a moderate fit to the data. If higher accuracy is desired, other machine learning algorithms such as regression models may be better tailored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "From the examples, lectures provided in class, as well as online tools and libraries.\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "In the order that they were listed.\n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "\n",
    "I did not use generative AI for this section. I used the fundamentals I learnt in part 1.\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "I didn't have as many challenges as I did in part 1. I think I found good resources in part 1 that allowed me to be more successful in part 2 as I had them readily available. These include online resources and the lecture notes + examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "\n",
    "The linear regression model, with a training Mean Squared Error of 110.66 and a Validation Mean Squared Error of 95.98, demonstrates a reasonable ability to understand the data's patterns. \n",
    "\n",
    "It generalizes well, as shown by the validation MSE being slightly lower than the training MSE. This balance between simplicity and accuracy, evident in the training R2 score of 0.610 and the validation R2 score of 0.628, aligns with our discussions on finding the right model trade-off in machine learning, where the linear regression model provides a fair fit for this dataset.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "\n",
    "I liked that we could relate the results to the real-life scenario of the dataset. It makes it more engaging to complete assignments when you can see the direct affect of the results.\n",
    "\n",
    "\n",
    "- found interesting, confusing, challangeing, motivating while working on this assignment.\n",
    "\n",
    "I found it a little challenging to figure out where the errors are occuring when the results aren't producing results that reinforce the patterns we learnt in class. It took quite a bit of researching to figure out.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best method: Lasso\n",
      "Best alpha: 10\n",
      "Best R2 score: 0.47935157993290034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "ridge_model = Ridge()\n",
    "lasso_model = Lasso()\n",
    "\n",
    "param_grid = {'alpha': alphas}\n",
    "\n",
    "ridge_grid = GridSearchCV(ridge_model, param_grid, cv=5, scoring='r2')\n",
    "ridge_grid.fit(X, y)\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso_model, param_grid, cv=5, scoring='r2')\n",
    "lasso_grid.fit(X, y)\n",
    "\n",
    "best_alpha_ridge = ridge_grid.best_params_['alpha']\n",
    "best_r2_score_ridge = ridge_grid.best_score_\n",
    "\n",
    "best_alpha_lasso = lasso_grid.best_params_['alpha']\n",
    "best_r2_score_lasso = lasso_grid.best_score_\n",
    "\n",
    "if best_r2_score_ridge > best_r2_score_lasso:\n",
    "    best_method = \"Ridge\"\n",
    "    best_alpha = best_alpha_ridge\n",
    "    best_r2_score = best_r2_score_ridge\n",
    "else:\n",
    "    best_method = \"Lasso\"\n",
    "    best_alpha = best_alpha_lasso\n",
    "    best_r2_score = best_r2_score_lasso\n",
    "\n",
    "print(f\"Best method: {best_method}\")\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best R2 score: {best_r2_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
